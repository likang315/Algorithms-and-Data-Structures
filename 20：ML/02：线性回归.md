### 线性回归

------

[TOC]

##### 01：线性回归（Linear Regression）

- 表示输入变量（特征）与连续输出变量（目标）之间的**线性关系模型**。
- 其核心思想是找到**一条最佳拟合直线（或超平面**），使得预测值与真实值之间的误差最小化。

###### 示例

- 根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 
- 数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为*训练数据集*（training data set） 或***训练集*（training set）**。 每行数据（比如一次房屋交易相对应的数据）称为***样本*（sample）**， 也可以称为*数据点*（data point）或*数据样本*（data instance）。 我们把试图**预测的目标（比如预测房屋价格）称为*标签*（label）或*目标*（target）**。 预测所依据的自变量（面积和房龄）称为**特征（feature）或协变量（covariate）**。

##### 02：线性模型

- 假设自变量 `x`和因变量`y`之间的关系是线性的， 即`y`可以表示为`x`中元素的加权和，这里通常允许**包含观测值的一些噪声**； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

###### 买房模型

- 目标（房屋价格）可以表示为特征（面积和房龄）的加权和。
  $$
  y = w1*x1 + w2*x2 + w3*x3 + b
  $$

  - x1，x2 因素，w1，w2权重。权重决定了每个特征对我们预测值的影响。 称为*偏移量*（offset）。
  - 通过加权和对特征进行**线性变换（linear transformation）**， 并通过偏移量来进行平移（translation）

###### 损失函数

- 量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。

- $$
  \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
  $$

  - 当样本`i`的预测值为`y^`，其相应的真实标签为`y`。

###### 单层神经网络

- x1、x2、x3 输入层，y 输出层

###### 神经元（Neural Network）

- <img src="/Users/likang/work/kungFu/Algorithms-and-Data-Structures/20：ML/photos/neural-network.png" alt="neural-network" style="zoom:30%;" />